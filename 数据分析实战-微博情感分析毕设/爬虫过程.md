# 爬虫的过程演示

目标网站:[微博](https://m.weibo.cn/)

# 首先寻找对应的API获取数据源

![image-20220216160911689](https://gitee.com/mqsnq30/gitee-table/raw/master/img/20220216160931.png)

![image-20220216161011550](https://gitee.com/mqsnq30/gitee-table/raw/master/img/20220216161011.png)

![image-20220216161111220](https://gitee.com/mqsnq30/gitee-table/raw/master/img/20220216161111.png)

![image-20220216161152155](https://gitee.com/mqsnq30/gitee-table/raw/master/img/20220216161152.png)

找到对应的API之后，查看是否是我们需要的

![image-20220216161227341](https://gitee.com/mqsnq30/gitee-table/raw/master/img/20220216161227.png)

内容完全对的上，说明这个就是我们需要的API文档，然后我们再去查看URL的构建

第一页是这样的

![image-20220216161309447](https://gitee.com/mqsnq30/gitee-table/raw/master/img/20220216161309.png)

我们要找多几个来对照

第二页

![image-20220216161345814](https://gitee.com/mqsnq30/gitee-table/raw/master/img/20220216161345.png)

第三页

![image-20220216161403966](https://gitee.com/mqsnq30/gitee-table/raw/master/img/20220216161404.png)

发现基本都一样，唯一要改的就是max_id

对应的，每个max_id，都在上一页的api数据里面

![image-20220216161541620](https://gitee.com/mqsnq30/gitee-table/raw/master/img/20220216161541.png)

，这样的话，发现规律之后，后面就容易了，根据这些对应的规则来构建每一条对应的URL

首先通过第一页的max_id来构建第二页的max_id

![image-20220216161656811](https://gitee.com/mqsnq30/gitee-table/raw/master/img/20220216161656.png)

 然后后面的页数都是按照这种规则

![image-20220216161717971](https://gitee.com/mqsnq30/gitee-table/raw/master/img/20220216161718.png)

然后通过获取到的json格式来获取相应的内容

![image-20220216161812513](https://gitee.com/mqsnq30/gitee-table/raw/master/img/20220216161812.png)

获取好之后保存到csv文件里面

，每爬到500页之后，把获取到的内容写入到CSV文件里面

![image-20220216161848889](https://gitee.com/mqsnq30/gitee-table/raw/master/img/20220216161848.png)

如果发生错误就用log把错误的位置记录下来

![image-20220216161910427](https://gitee.com/mqsnq30/gitee-table/raw/master/img/20220216161910.png)

，然后再继续爬即可



以上便是爬虫的全部过程