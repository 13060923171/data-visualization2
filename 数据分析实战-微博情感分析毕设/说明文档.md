## 微博舆论分析项目

# 1、数据获取项目

数据获取是通过![image-20220130135737605](https://cdn.jsdelivr.net/gh/13060923171/images@main/img/image-20220130135737605.png)

这个爬虫程序去获取的

网站是这个[手机版网页](https://m.weibo.cn/)

然后去获取首先是网页的检查这边去获取对应的id账号

![image-20220130140046780](https://cdn.jsdelivr.net/gh/13060923171/images@main/img/image-20220130140046780.png)

然后把这个位置修改成想要获取到的id账号

![image-20220130140116324](https://cdn.jsdelivr.net/gh/13060923171/images@main/img/image-20220130140116324.png)

然后去获取对应的API文件

![image-20220130140144481](https://cdn.jsdelivr.net/gh/13060923171/images@main/img/image-20220130140144481.png)

根据他的第一页的内容获取下一页的内容

然后重复该过程

![image-20220130140217200](https://cdn.jsdelivr.net/gh/13060923171/images@main/img/image-20220130140217200.png)

最后获取到的内容

![image-20220130140245238](https://cdn.jsdelivr.net/gh/13060923171/images@main/img/image-20220130140245238.png)

用pandas写入csv文件里面

# 2、数据预处理

先把全部的内容汇集到一个dataframe文件里面

![image-20220130140441667](https://cdn.jsdelivr.net/gh/13060923171/images@main/img/image-20220130140441667.png)

然后去除空值和重复项内容

![image-20220130140508603](https://cdn.jsdelivr.net/gh/13060923171/images@main/img/image-20220130140508603.png)

删除完之后从 4万多条数据变成1万多条数据

![image-20220130140540028](https://cdn.jsdelivr.net/gh/13060923171/images@main/img/image-20220130140540028.png)

然后再删除一些标点符号

![image-20220130140609786](https://cdn.jsdelivr.net/gh/13060923171/images@main/img/image-20220130140609786.png)

然后采用机械压缩把文本内容进行压缩

![image-20220130140639212](https://cdn.jsdelivr.net/gh/13060923171/images@main/img/image-20220130140639212.png)

然后我们再用停用词和分词进行高频词统计

![image-20220130140732147](https://cdn.jsdelivr.net/gh/13060923171/images@main/img/image-20220130140732147.png)

![image-20220130140739475](https://cdn.jsdelivr.net/gh/13060923171/images@main/img/image-20220130140739475.png)

![image-20220130140749698](https://cdn.jsdelivr.net/gh/13060923171/images@main/img/image-20220130140749698.png)

# 3、数据分析

我们来计算它的tf-idf的值，进行聚类可视化来查看

先计算好它的tf-idf

![image-20220130140908214](https://cdn.jsdelivr.net/gh/13060923171/images@main/img/image-20220130140908214.png)

计算好之后用聚类的可视化来进行查看

![image-20220130140941293](https://cdn.jsdelivr.net/gh/13060923171/images@main/img/image-20220130140941293.png)

![image-20220130140953749](https://cdn.jsdelivr.net/gh/13060923171/images@main/img/image-20220130140953749.png)

然后我们再用snownlp来进行数据分类，把正面和负面的评论进行区分出来

顺便把它们的训练集和测试集区分出来

![image-20220130141054054](https://cdn.jsdelivr.net/gh/13060923171/images@main/img/image-20220130141054054.png)

![image-20220130141112190](https://cdn.jsdelivr.net/gh/13060923171/images@main/img/image-20220130141112190.png)

![image-20220130141231819](https://cdn.jsdelivr.net/gh/13060923171/images@main/img/image-20220130141231819.png)

然后我们用机器学习中的贝叶斯进行数据分类

![image-20220130141305084](https://cdn.jsdelivr.net/gh/13060923171/images@main/img/image-20220130141305084.png)

![image-20220130141326416](https://cdn.jsdelivr.net/gh/13060923171/images@main/img/image-20220130141326416.png)

预测出来的准确率为0.88，准确率一般在0.8以上都是处于较好的训练模型，所以这个模型已经算是比较可以的了



# 数据可视化

最后我们根据我们上面处理的数据进行数据可视化

词云图

![image-20220130141505533](https://cdn.jsdelivr.net/gh/13060923171/images@main/img/image-20220130141505533.png)

![image-20220130141534421](https://cdn.jsdelivr.net/gh/13060923171/images@main/img/image-20220130141534421.png)

饼图

![image-20220130141605417](https://cdn.jsdelivr.net/gh/13060923171/images@main/img/image-20220130141605417.png)

![image-20220130141621015](https://cdn.jsdelivr.net/gh/13060923171/images@main/img/image-20220130141621015.png)

查看正面和负面之间的占比情况



时间趋势图

![image-20220130141658078](https://cdn.jsdelivr.net/gh/13060923171/images@main/img/image-20220130141658078.png)

![image-20220130141712998](https://cdn.jsdelivr.net/gh/13060923171/images@main/img/image-20220130141712998.png)



人物领袖图

![image-20220130141746961](https://cdn.jsdelivr.net/gh/13060923171/images@main/img/image-20220130141746961.png)

![image-20220130141801576](https://cdn.jsdelivr.net/gh/13060923171/images@main/img/image-20220130141801576.png)

