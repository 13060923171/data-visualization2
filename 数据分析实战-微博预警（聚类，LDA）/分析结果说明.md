# 数据储存部分说明

数据存储设计，首先我们设计一个dowload函数用于储存

![image-20220410104246183](https://cdn.jsdelivr.net/gh/13060923171/images@main/img/image-20220410104246183.png)

然后根据这些获取到信息，我们用字典的形式保存起来

![image-20220410104322878](https://cdn.jsdelivr.net/gh/13060923171/images@main/img/image-20220410104322878.png)

![image-20220410104352006](https://cdn.jsdelivr.net/gh/13060923171/images@main/img/image-20220410104352006.png)

并且写好的字典内容，用列表的形式保存起来

传到一个总的列表里面

![image-20220410104441226](https://cdn.jsdelivr.net/gh/13060923171/images@main/img/image-20220410104441226.png)



然后进行一个循环防错处理

当爬到尾页的时候我们就停下来，如果没有那么多页我们就把这些数据保存起来，用CSV的格式，如果出现错误，直接把数据保存起来，防止数据丢失的问题

![image-20220410104556174](https://cdn.jsdelivr.net/gh/13060923171/images@main/img/image-20220410104556174.png)



最后每爬取一页，就做一个内容的保存，这样一来不会让数据丢失，二来也缓解了内存的压力

，方法用的是,mode="a+"的形式，也就说不断的追加形式，确保新的内容不会把旧的内容刷新，做到不断往表格里面添加新的数据，并且旧的数据也能被保留下来

# 以下分析步骤主要分为三个部分：

## 第一步部分，数据处理部分：

首先这是原始数据的部分：

![image-20220410084535928](https://cdn.jsdelivr.net/gh/13060923171/images@main/img/image-20220410084535928.png)

根据这些数据，我们要做一个大量的清洗步骤，这样才能确认我们后期数据分析的部分能够顺利做好

1. 我们要删除一个无效数据列，这些列并没有什么分析的价值，像第三列和最后一列其实并没有任何意义，这些就是无效列，我们要全部删除
2. 第二我们因为要分析时间趋势这些时间其实并不能准确的告诉我们时间的发生变化，所以我们还是要对其进行处理，把时间格式全部统一，并且替换成日期的形式
3. 在评论内容这边，有很多的无效字符，比如是【 这些无效字符，这些字符也会影响我们后期分析内容的结果，所以也是要删除的
4. 根据转发，评论，点赞，我们把这些中文字符转化为0
5. 再把空行全部删除
6. 最后我们进行一个情感分析的判断，去判断每个评论对应的情感分析，这里采用的是百度开源的senta_bilstm模型，这个是成熟NLP模型便于我们进行更为精准的计算，得出较为正确的分数，越接近1说明情感倾向偏正，越接近0说明情感倾向偏负

处理好的文本如下：

![image-20220410085357564](https://cdn.jsdelivr.net/gh/13060923171/images@main/img/image-20220410085357564.png)





## 第二步部分，预警分析

首先根据这个微博数量的变化趋势，我们可以判断舆情目前的一个大致走向

![疫情的微博数量变化趋势](https://cdn.jsdelivr.net/gh/13060923171/images@main/img/%E7%96%AB%E6%83%85%E7%9A%84%E5%BE%AE%E5%8D%9A%E6%95%B0%E9%87%8F%E5%8F%98%E5%8C%96%E8%B6%8B%E5%8A%BF.png)

从这边我们不难看出，微博数量的变化，呈往上走的趋势，尤其是在第二天突然从几十条评论变成几百条评论，这时就已经到了我们的达到了我们的预警范围了，我们就要开始逐步对其进行一个详细的分析判断流程，去查看为什么在这一天突然变化如此之大，导致这个变化的原因是什么，是什么其他开始爆发了疫情，引起了人们激烈的讨论。



这时我们先头开始分析，首先先查看微博，人们的具体反应情况如下，单单看发帖数量不足以说明什么，还得查看人们的互动情况如何，从这些互动的情况，更能反应出，人们对疫情目前关心的程度如何

![疫情的关注度变化趋势](https://cdn.jsdelivr.net/gh/13060923171/images@main/img/%E7%96%AB%E6%83%85%E7%9A%84%E5%85%B3%E6%B3%A8%E5%BA%A6%E5%8F%98%E5%8C%96%E8%B6%8B%E5%8A%BF.png)

从该图看出，人们点赞率远远高于评论和转发，说明人们心里对于这些发帖人的发帖内容还是很认可他们的内容的，从而也反应出这次人们的关注程度，点赞的数量从3万多到12万左右，这些也间接说明，这个讨论的趋势越是越来越大，也达到了预警的范围，接着我们再去对后面，进行进一步探讨



![微博-疫情-关注度最高的情感倾向](https://cdn.jsdelivr.net/gh/13060923171/images@main/img/%E5%BE%AE%E5%8D%9A-%E7%96%AB%E6%83%85-%E5%85%B3%E6%B3%A8%E5%BA%A6%E6%9C%80%E9%AB%98%E7%9A%84%E6%83%85%E6%84%9F%E5%80%BE%E5%90%91.png)

这个是每天微博，关注度最高的那条微博的内容的一个情感倾向的判断，这里关注度最高是首先筛选，每天的微博，对其进行归类，然后再根据那天微博的评论内容，来判断这个是人们最关心的话题，因为单纯转发和点赞，其实并不能有一个强有力的说明，只有当人们都参与谈论了，这样才能证明这个人们最关心的话题，也是这样更容易来对其进行一个分析和判断

如图所示，这几天的情感倾向由原来正面突然转变为负面，很接近0那种，根据上面，转折点也是从第二天开始，第二天发帖数量突然暴涨，人们点赞数量也开始暴涨，说明这一天疫情的发展趋势，严重影响了人们的生活质量，导致大家都去到网上来宣泄自己的情感，这时全是负面的影响，符合了我们的预警，我们这时就要开始去探讨，人们不满的地方在哪里，去逐一分析判断，来缓解舆情所带来的负面影响



我们去查看我们的lda主题模型

这里我们先是做好了去重的处理和删除一些无效词

![image-20220410092324084](https://cdn.jsdelivr.net/gh/13060923171/images@main/img/image-20220410092324084.png)

处理好的文本也从868的数量到了706的数量，删除了很多无效的文本，让我们后续的分析变为准确

接着我们进行分词处理，并且采用tf-idf算法进行文本处理

![image-20220410092504930](https://cdn.jsdelivr.net/gh/13060923171/images@main/img/image-20220410092504930.png)

处理好的词之后我们再进行用LDA主题分析去查阅，每个主题下，哪些词影响最大

![image-20220410092609995](https://cdn.jsdelivr.net/gh/13060923171/images@main/img/image-20220410092609995.png)

在我们的主题一中，主要是围绕整个疫情，防控，肺炎这些来进行一个探讨，这是一个大的主题

![image-20220410092653277](https://cdn.jsdelivr.net/gh/13060923171/images@main/img/image-20220410092653277.png)

![image-20220410092731873](https://cdn.jsdelivr.net/gh/13060923171/images@main/img/image-20220410092731873.png)

在主题二中，则是围绕着上海，北京，广州，这些个个地区的防疫情况如何，

而主题三中，则是人们的一些日常生活，比如出行，电话，春天这种，这个也说明疫情对人们造成的生活影响情况如何

![image-20220410092821908](https://cdn.jsdelivr.net/gh/13060923171/images@main/img/image-20220410092821908.png)



## 第三步部分，行为分析

在这里我们主要是去查看这些分类后的情况，把这些分类结果，去判断人的一个具体行为

这里我们采用的是k-means模型，去进行一个聚类分析

首先还是使用我们的tf-idf算法去计算每个词的权重

![image-20220410093224864](https://cdn.jsdelivr.net/gh/13060923171/images@main/img/image-20220410093224864.png)

接着再根据这些权重处理好的值

![image-20220410093242286](https://cdn.jsdelivr.net/gh/13060923171/images@main/img/image-20220410093242286.png)

进行一个聚类效果，这边根据可视化，聚3类的效果不错

![词性聚类图](https://cdn.jsdelivr.net/gh/13060923171/images@main/img/%E8%AF%8D%E6%80%A7%E8%81%9A%E7%B1%BB%E5%9B%BE.jpg)

这里可以明显的看出，除了少量的黄球和绿球混在一起了，其他的都完全分开了，如果是聚成其他类，那效果反而更差，因此这些，聚三类即可

聚好的文本如下：

![image-20220410093521696](https://cdn.jsdelivr.net/gh/13060923171/images@main/img/image-20220410093521696.png)

然后我们去探讨每一类所代表的含义是什么，这样我们也方便后续的归类问题

因为聚类是无监督学习的，也就是每次聚类的结果都不一样，所以我们不能把这些聚类说死，最好的归为便是聚类1，聚类2，聚类3,具体的背后含义，则是去查看它们对应的词云图去判断它们是什么

然后再去下判断



词云图一，主要则是针对上海的一个确诊情况和新增病例情况

![聚类1-云图](D:/photo/聚类1-云图.png)



词云图二则是上海疫情防控的情况如何

![聚类2-云图](https://cdn.jsdelivr.net/gh/13060923171/images@main/img/%E8%81%9A%E7%B1%BB2-%E4%BA%91%E5%9B%BE.png)

词云图三则是广州，上海，无锡，这些个个地名疫情防控的情况

![聚类3-云图](https://cdn.jsdelivr.net/gh/13060923171/images@main/img/%E8%81%9A%E7%B1%BB3-%E4%BA%91%E5%9B%BE.png)

![分类数量统计](https://cdn.jsdelivr.net/gh/13060923171/images@main/img/%E5%88%86%E7%B1%BB%E6%95%B0%E9%87%8F%E7%BB%9F%E8%AE%A1.jpg)

这边便是它们聚类后的数量统计，其中聚类1数量是最多的，也就说目前微博讨论的主要内容则是上海疫情防控的情况如何，也符合我们的目前的形式判断。